{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo, dqn\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import get_config, setup_ray, save, load\n",
    "setup_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = dict(\n",
    "    fleetsize = 2,\n",
    "    max_fleetsize = 30,    \n",
    "    pseudo_dispatcher = True,\n",
    "    routing_agent_death= True,\n",
    "    sim_config = dict(\n",
    "        dispatch = True,\n",
    "        routing_ma = True,\n",
    "        dispatching_ma = True,\n",
    "        reward_reached_target = 1, \n",
    "        reward_wrong_target = -0.5,\n",
    "        reward_removed_for_block = -3, \n",
    "        reward_target_distance = -0.05,\n",
    "        reward_invalid=-0.1,\n",
    "        block_timeout = 20,\n",
    "        reward_accepted_in_station = 1,\n",
    "        reward_declined_in_station = -1,\n",
    "        dispatching_interval=360,\n",
    "        io_quote = 0.99  ,\n",
    "        availability = 0.95,\n",
    "        mttr = 5*60,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agv_model = dict(\n",
    "    model = dict(\n",
    "        custom_model = \"lin_model\",\n",
    "        custom_action_dist=\"MAActionDistribution\",\n",
    "        custom_model_config = dict(\n",
    "            embed_dim=32,\n",
    "            with_action_mask=True,\n",
    "            with_agvs=True,\n",
    "            with_stations = False,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "dispatcher_model = dict(\n",
    "    model = dict(\n",
    "        custom_model = \"lin_model\",\n",
    "        custom_action_dist=\"MAActionDistribution\",\n",
    "        custom_model_config = dict(\n",
    "            embed_dim=16,\n",
    "            with_action_mask=False,\n",
    "            with_agvs=True,\n",
    "            with_stations = True,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 18:01:11,828\tWARNING ppo.py:386 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1250.\n",
      "2022-11-08 18:01:11,829\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-11-08 18:01:11,830\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_75083_r93lvslq)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-11-08 18:01:24,396\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-11-08 18:01:30,228\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-11-08 18:01:31,627\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-11-08 18:01:32,895\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-11-08 18:01:32,903\tINFO trainable.py:159 -- Trainable.setup took 21.081 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-11-08 18:01:32,908\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config, logger_creator, checkpoint_dir = get_config(\n",
    "    batch_size=5000,\n",
    "    env_args = env_args, \n",
    "    agv_model = agv_model,\n",
    "    train_agv = True,\n",
    "    dispatcher_model=dispatcher_model, \n",
    "    train_dispatcher=True,\n",
    "    env = \"matrix\"\n",
    ")\n",
    "trainer = ppo.PPOTrainer(config, logger_creator=logger_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.restore(\"../../models/Default/2_30_2022-11-07_22-32-06/checkpoint_000100/checkpoint-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(500):\n",
    "    for i in range(20):\n",
    "        trainer.train()    \n",
    "    trainer.save(checkpoint_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
