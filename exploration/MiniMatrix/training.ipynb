{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 12:48:57,213\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import setup_ray, save, load, Experiment\n",
    "path = \"D:/Master/Masterarbeit/thesis\"\n",
    "setup_ray(path = path, unidirectional = False, seed=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = dict(\n",
    "    fleetsize = 2,\n",
    "    max_fleetsize = 10,    \n",
    "    pseudo_routing = False,\n",
    "    pseudo_dispatcher = True,\n",
    "    #pseudo_dispatcher_distance = 0.2,\n",
    "    routing_agent_death= False,\n",
    "    death_on_target = False,\n",
    "    sim_config = dict(\n",
    "        dispatch = True,\n",
    "        routing_ma = True,\n",
    "        dispatching_ma = True,\n",
    "        reward_reached_target = 1,\n",
    "        #reward_reached_target_by_time = True, \n",
    "        reward_wrong_target = -0.5,\n",
    "        reward_removed_for_block = -1, \n",
    "        #reward_target_distance = -0.05,\n",
    "        reward_invalid= -0.1,\n",
    "        block_timeout = 120,\n",
    "        reward_accepted_in_station = 1,\n",
    "        reward_declined_in_station = -1,\n",
    "        routing_interval = 2,\n",
    "        dispatching_interval=360,\n",
    "        io_quote = 0.99  ,\n",
    "        availability = 0.95,\n",
    "        mttr = 360,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agv_model = dict(\n",
    "    model = dict(\n",
    "        custom_model = \"lin_model\",\n",
    "        #custom_action_dist=\"MAActionDistribution\",\n",
    "        custom_model_config = dict(\n",
    "            embed_dim=32,\n",
    "            with_action_mask=False,\n",
    "            with_agvs=True,\n",
    "            with_stations = False,\n",
    "            pos_embedd_dim = 0,\n",
    "            ff_embedd_dim = 0,\n",
    "            #env_type = \"matrix\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "dispatcher_model = dict(\n",
    "    model = dict(\n",
    "        custom_model = \"lin_model\",\n",
    "        #custom_action_dist=\"MAActionDistribution\",\n",
    "        custom_model_config = dict(\n",
    "            embed_dim=32,\n",
    "            with_action_mask=False,\n",
    "            with_agvs=True,\n",
    "            with_stations = True,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 12:49:07,470\tINFO simple_q.py:307 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "2022-12-25 12:49:07,540\tINFO algorithm.py:457 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_53411_ffzc5e7c)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_53430_0x27qkdc)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_53436_6feyvto5)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_53443_n088e54u)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-12-25 12:49:57,917\tINFO trainable.py:164 -- Trainable.setup took 50.467 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=8460)\u001b[0m 2022-12-25 12:50:12,253\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=8460)\u001b[0m 2022-12-25 12:50:12,254\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=14640)\u001b[0m 2022-12-25 12:50:21,903\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=14640)\u001b[0m 2022-12-25 12:50:21,905\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=2008)\u001b[0m 2022-12-25 12:50:37,099\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=2008)\u001b[0m 2022-12-25 12:50:37,100\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=2480)\u001b[0m 2022-12-25 12:50:46,624\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=2480)\u001b[0m 2022-12-25 12:50:46,624\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\ray\\rllib\\utils\\metrics\\window_stat.py:50: RuntimeWarning: Mean of empty slice\n",
      "  return float(np.nanmean(self.items[: self.count]))\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=2008)\u001b[0m 2022-12-25 13:05:52,474\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=8460)\u001b[0m 2022-12-25 13:06:10,618\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=14640)\u001b[0m 2022-12-25 13:06:27,965\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=2480)\u001b[0m 2022-12-25 13:06:46,683\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_61389_rk2nievu)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_61394_3jcmf58k)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_61396_1l1yqsoy)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_61400_tu7_p65_)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-12-25 14:44:26,958\tINFO trainable.py:164 -- Trainable.setup took 26.026 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=492)\u001b[0m 2022-12-25 14:44:33,390\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=492)\u001b[0m 2022-12-25 14:44:33,390\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=5976)\u001b[0m 2022-12-25 14:44:37,013\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=5976)\u001b[0m 2022-12-25 14:44:37,013\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=13420)\u001b[0m 2022-12-25 14:44:40,919\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=13420)\u001b[0m 2022-12-25 14:44:40,919\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=13580)\u001b[0m 2022-12-25 14:45:02,760\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0074025 GB (2500.0 batches of size 1, 2961 bytes each), available system memory is 12.29264896 GB\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=13580)\u001b[0m 2022-12-25 14:45:02,760\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\ray\\rllib\\utils\\metrics\\window_stat.py:50: RuntimeWarning: Mean of empty slice\n",
      "  return float(np.nanmean(self.items[: self.count]))\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis3_9\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=13420)\u001b[0m 2022-12-25 15:01:03,405\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=5976)\u001b[0m 2022-12-25 15:01:22,643\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=492)\u001b[0m 2022-12-25 15:01:43,424\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MultiAgentPrioritizedReplayBuffer pid=13580)\u001b[0m 2022-12-25 15:02:04,119\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(\"minimatrix_routing\")\n",
    "for seed in [42,43,44]:\n",
    "    exp.experiment(\n",
    "        path = path,\n",
    "        env_args = env_args, \n",
    "        agv_model = agv_model,\n",
    "        dispatcher_model=dispatcher_model,\n",
    "        run_name=\"02_algorithmus_rainbow_\", \n",
    "        env = \"minimatrix\",\n",
    "        algo = \"rainbow\",\n",
    "        n_intervals =4,\n",
    "        backup_interval=50,\n",
    "        #batch_size=50, #apex + gnn: 50\n",
    "        seed = seed,\n",
    "        #load_agv=\"../../models/first_experiments/05_ff__2_30_2022-12-20_16-52-40/checkpoint_000300/\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('thesis3_9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0516e323c1d6337405feeccc202b0dbcb07dc1a4aafa5eedf3cd6ee0d411108"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
