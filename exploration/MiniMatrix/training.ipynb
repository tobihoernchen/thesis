{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo, dqn\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import get_config, setup_ray, save, load\n",
    "setup_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = dict(\n",
    "    fleetsize = 2,\n",
    "    max_fleetsize = 30,    \n",
    "    pseudo_dispatcher = True,\n",
    "    routing_agent_death= True,\n",
    "    sim_config = dict(\n",
    "        dispatch = True,\n",
    "        routing_ma = True,\n",
    "        dispatching_ma = True,\n",
    "        reward_reached_target = 1, \n",
    "        reward_wrong_target = -0.3,\n",
    "        reward_removed_for_block = -3, \n",
    "        reward_target_distance = 0.2,\n",
    "        reward_invalid=-0.1,\n",
    "        block_timeout = 20,\n",
    "        reward_accepted_in_station = 1,\n",
    "        reward_declined_in_station = -1,\n",
    "        dispatching_interval=240,\n",
    "        io_quote = 0.99  ,\n",
    "        availability = 0.95,\n",
    "        mttr = 5*60,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agv_model = dict(\n",
    "    model = dict(\n",
    "        custom_model = \"lin_model\",\n",
    "        custom_action_dist=\"MAActionDistribution\",\n",
    "        custom_model_config = dict(\n",
    "            embed_dim=32,\n",
    "            with_action_mask=True,\n",
    "            with_agvs=True,\n",
    "            with_stations = False,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "dispatcher_model = dict(\n",
    "    model = dict(\n",
    "        custom_model = \"lin_model\",\n",
    "        custom_action_dist=\"MAActionDistribution\",\n",
    "        custom_model_config = dict(\n",
    "            embed_dim=16,\n",
    "            with_action_mask=False,\n",
    "            with_agvs=True,\n",
    "            with_stations = True,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 21:15:29,916\tWARNING ppo.py:386 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1250.\n",
      "2022-11-07 21:15:29,917\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-11-07 21:15:29,920\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_88576_98km8hr9)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n"
     ]
    }
   ],
   "source": [
    "config, logger_creator, checkpoint_dir = get_config(\n",
    "    batch_size=5000,\n",
    "    env_args = env_args, \n",
    "    agv_model = agv_model,\n",
    "    train_agv = True,\n",
    "    dispatcher_model=dispatcher_model, \n",
    "    train_dispatcher=True,\n",
    "    env = \"matrix\"\n",
    ")\n",
    "trainer = ppo.PPOTrainer(config, logger_creator=logger_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.restore(\"../../models/Default/1_10_2022-11-07_17-38-06/checkpoint_000020/checkpoint-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "ModelError[path=, status={'reason': 'MODEL_ERROR Caused by class java.lang.ArrayIndexOutOfBoundsException: Index 24 out of bounds for length 24', 'next_event_time': 480.1, 'episode_count': 4, 'step_count': 435, 'model_time': 480.1}, error=None, msg='']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\training.ipynb Zelle 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/training.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/training.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/training.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         trainer\u001b[39m.\u001b[39;49mtrain()    \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/training.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     trainer\u001b[39m.\u001b[39msave(checkpoint_dir)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\tune\\trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warmup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[0;32m    359\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 360\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    361\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1136\u001b[0m, in \u001b[0;36mTrainer.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1133\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1134\u001b[0m             \u001b[39m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m             time\u001b[39m.\u001b[39msleep(\u001b[39m0.5\u001b[39m)\n\u001b[1;32m-> 1136\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m   1138\u001b[0m result \u001b[39m=\u001b[39m step_attempt_results\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mworkers\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers, WorkerSet):\n\u001b[0;32m   1141\u001b[0m     \u001b[39m# Sync filters on workers.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m step_ctx\u001b[39m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[0;32m   1110\u001b[0m     \u001b[39m# Try to train one step.\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m         step_attempt_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_attempt()\n\u001b[0;32m   1113\u001b[0m     \u001b[39m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m     \u001b[39mexcept\u001b[39;00m RayError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1115\u001b[0m         \u001b[39m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[39m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evaluate_this_iter:\n\u001b[1;32m-> 1214\u001b[0m     step_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec_plan_or_training_iteration_fn()\n\u001b[0;32m   1215\u001b[0m \u001b[39m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1217\u001b[0m     \u001b[39m# No parallelism.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mevaluation_parallel_to_training\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:2209\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2207\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2208\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_disable_execution_plan_api\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m-> 2209\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_iteration()\n\u001b[0;32m   2210\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2211\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\ppo\\ppo.py:433\u001b[0m, in \u001b[0;36mPPOTrainer.training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m@ExperimentalAPI\u001b[39m\n\u001b[0;32m    430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_iteration\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResultDict:\n\u001b[0;32m    431\u001b[0m     \u001b[39m# Collect SampleBatches from sample workers until we have a full batch.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_by_agent_steps:\n\u001b[1;32m--> 433\u001b[0m         train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[0;32m    434\u001b[0m             worker_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers, max_agent_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m    435\u001b[0m         )\n\u001b[0;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[0;32m    438\u001b[0m             worker_set\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers, max_env_steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mtrain_batch_size\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    439\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py:96\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[1;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mwhile\u001b[39;00m (max_agent_or_env_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m agent_or_env_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m     90\u001b[0m     max_agent_or_env_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     \u001b[39mand\u001b[39;00m agent_or_env_steps \u001b[39m<\u001b[39m max_agent_or_env_steps\n\u001b[0;32m     92\u001b[0m ):\n\u001b[0;32m     93\u001b[0m     \u001b[39m# No remote workers in the set -> Use local worker for collecting\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[39m# samples.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m worker_set\u001b[39m.\u001b[39mremote_workers():\n\u001b[1;32m---> 96\u001b[0m         sample_batches \u001b[39m=\u001b[39m [worker_set\u001b[39m.\u001b[39;49mlocal_worker()\u001b[39m.\u001b[39;49msample()]\n\u001b[0;32m     97\u001b[0m     \u001b[39m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m         sample_batches \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mget(\n\u001b[0;32m    100\u001b[0m             [worker\u001b[39m.\u001b[39msample\u001b[39m.\u001b[39mremote() \u001b[39mfor\u001b[39;00m worker \u001b[39min\u001b[39;00m worker_set\u001b[39m.\u001b[39mremote_workers()]\n\u001b[0;32m    101\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:825\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39msample_start\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m    820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGenerating sample batch of size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    821\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_fragment_length\n\u001b[0;32m    822\u001b[0m         )\n\u001b[0;32m    823\u001b[0m     )\n\u001b[1;32m--> 825\u001b[0m batches \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_reader\u001b[39m.\u001b[39;49mnext()]\n\u001b[0;32m    826\u001b[0m steps_so_far \u001b[39m=\u001b[39m (\n\u001b[0;32m    827\u001b[0m     batches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcount\n\u001b[0;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_steps_by \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39menv_steps\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    829\u001b[0m     \u001b[39melse\u001b[39;00m batches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39magent_steps()\n\u001b[0;32m    830\u001b[0m )\n\u001b[0;32m    832\u001b[0m \u001b[39m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[39m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py:115\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39m@override\u001b[39m(InputReader)\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SampleBatchType:\n\u001b[1;32m--> 115\u001b[0m     batches \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_data()]\n\u001b[0;32m    116\u001b[0m     batches\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extra_batches())\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batches) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py:288\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m@override\u001b[39m(SamplerInput)\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SampleBatchType:\n\u001b[0;32m    287\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_env_runner)\n\u001b[0;32m    289\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[0;32m    290\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics_queue\u001b[39m.\u001b[39mput(item)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py:721\u001b[0m, in \u001b[0;36m_env_runner\u001b[1;34m(worker, base_env, extra_batch_callback, horizon, normalize_actions, clip_actions, multiple_episodes_in_batch, callbacks, perf_stats, soft_horizon, no_done_at_end, observation_fn, sample_collector, render)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[39m# Return computed actions to ready envs. We also send to envs that have\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[39m# taken off-policy actions; those envs are free to ignore the action.\u001b[39;00m\n\u001b[0;32m    720\u001b[0m t4 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 721\u001b[0m base_env\u001b[39m.\u001b[39;49msend_actions(actions_to_send)\n\u001b[0;32m    722\u001b[0m perf_stats\u001b[39m.\u001b[39menv_wait_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t4\n\u001b[0;32m    724\u001b[0m \u001b[39m# Try to render the env, if required.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py:524\u001b[0m, in \u001b[0;36mMultiAgentEnvWrapper.send_actions\u001b[1;34m(self, action_dict)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEnv \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is already done\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(env_id))\n\u001b[0;32m    523\u001b[0m env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_id]\n\u001b[1;32m--> 524\u001b[0m obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(agent_dict)\n\u001b[0;32m    525\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mNot a multi-agent obs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(rewards, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mNot a multi-agent reward\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py:108\u001b[0m, in \u001b[0;36mPettingZooEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49magent_selection])\n\u001b[0;32m    109\u001b[0m     obs_d \u001b[39m=\u001b[39m {}\n\u001b[0;32m    110\u001b[0m     rew_d \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\envs\\matrix.py:78\u001b[0m, in \u001b[0;36mMatrix.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatching_agent_death:\n\u001b[0;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatching_aliases_rev[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection]\n\u001b[1;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\envs\\base_alpyne_zoo.py:131\u001b[0m, in \u001b[0;36mBaseAlpyneZoo.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    127\u001b[0m alpyne_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_behaviors[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection]\u001b[39m.\u001b[39mconvert_to_action(\n\u001b[0;32m    128\u001b[0m     action, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection\n\u001b[0;32m    129\u001b[0m )\n\u001b[0;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msim\u001b[39m.\u001b[39mtake_action(alpyne_action)\n\u001b[1;32m--> 131\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collect()\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\envs\\base_alpyne_zoo.py:176\u001b[0m, in \u001b[0;36mBaseAlpyneZoo._collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_collect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    175\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mwait_for_completion()\n\u001b[0;32m    177\u001b[0m         alpyne_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msim\u001b[39m.\u001b[39mget_observation()\n\u001b[0;32m    178\u001b[0m         agent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_selection_fn(alpyne_obs)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\model_run.py:118\u001b[0m, in \u001b[0;36mModelRun.wait_for_completion\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mwhile\u001b[39;00m status \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [RunStatus\u001b[39m.\u001b[39mPAUSED, RunStatus\u001b[39m.\u001b[39mCOMPLETED, RunStatus\u001b[39m.\u001b[39mSTOPPED]:\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m==\u001b[39m RunStatus\u001b[39m.\u001b[39mFAILED:\n\u001b[1;32m--> 118\u001b[0m         \u001b[39mraise\u001b[39;00m ModelError(info)\n\u001b[0;32m    119\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolling_period)\n\u001b[0;32m    120\u001b[0m     status, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_state()\n",
      "\u001b[1;31mModelError\u001b[0m: ModelError[path=, status={'reason': 'MODEL_ERROR Caused by class java.lang.ArrayIndexOutOfBoundsException: Index 24 out of bounds for length 24', 'next_event_time': 480.1, 'episode_count': 4, 'step_count': 435, 'model_time': 480.1}, error=None, msg='']"
     ]
    }
   ],
   "source": [
    "for j in range(500):\n",
    "    for i in range(20):\n",
    "        trainer.train()    \n",
    "    trainer.save(checkpoint_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
