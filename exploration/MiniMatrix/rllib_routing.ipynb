{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo, dqn\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import save_hparams\n",
    "from thesis.utils.rllib_utils import rllib_ppo_config, setup_ray\n",
    "setup_ray(env = \"Routing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 17:09:00,174\tWARNING ppo.py:386 -- `train_batch_size` (7000) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1750.\n",
      "2022-09-26 17:09:00,175\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-09-26 17:09:00,177\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_71458_ub1eq7i8)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-09-26 17:09:11,318\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-26 17:09:18,194\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-26 17:09:20,571\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-26 17:09:22,410\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-26 17:09:22,416\tINFO trainable.py:159 -- Trainable.setup took 22.272 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-09-26 17:09:22,421\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "fleetsize = 6\n",
    "max_fleetsize = 10\n",
    "config_args = dict(\n",
    "    withCollisions = True,\n",
    "    reward_target = 1, \n",
    "    reward_distance = -0.1,\n",
    "    reward_block = -1, \n",
    "    reward_invalid=-0.3,\n",
    "    dispatchinginterval=120,\n",
    "    routinginterval = 2,\n",
    ")\n",
    "\n",
    "dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "trainer = ppo.PPOTrainer(rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations=0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 17:09:22,821\tINFO trainable.py:588 -- Restored on 127.0.0.1 from checkpoint: ../../models/Default/6-10-25_09-23_26_20/checkpoint_000112/checkpoint-112\n",
      "2022-09-26 17:09:22,823\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 112, '_timesteps_total': None, '_time_total': 80197.72135829926, '_episodes_total': 503}\n"
     ]
    }
   ],
   "source": [
    "trainer.restore(\"../../models/Default/6-10-25_09-23_26_20/checkpoint_000112/checkpoint-112\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\models\\modelv2.py:444: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if len(obs.shape) < 2 or obs.shape[-1] != prep.shape[0]:\n",
      "d:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\policies\\simplified_attention_module.py:28: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = x.view(x.shape[0], x.shape[1] // self.original_dim, self.original_dim)\n",
      "d:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\policies\\simplified_attention_module.py:113: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  n_other_agvs = agvs_data.shape[1] // self.n_features\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\jit\\_trace.py:958: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
      "  module._c._create_method_from_trace(\n"
     ]
    }
   ],
   "source": [
    "trainer.workers.local_worker().get_policy(\"agv\").export_model(\"../../models/RLLIB_POLICIES/Routing_V0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../models/Default/6-10-26_09-09_45_21\\\\checkpoint_000112\\\\checkpoint-112'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save(f\"{dir}/{run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(500):\n",
    "    for i in range(20):\n",
    "        trainer.train()    \n",
    "    trainer.save(f\"{dir}/{run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = rllib_ppo_config(fleetsize, max_fleetsize, config_args, entropy_coeff = 0.1, gamma = 0.8)\n",
    "# config[\"kl_coeff\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 21:06:39,267\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-08-30 21:06:39,268\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_87962_iq9rk6nm)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-08-30 21:06:46,929\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:52,643\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:54,417\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:56,063\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:56,066\tINFO trainable.py:159 -- Trainable.setup took 16.802 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-30 21:06:56,069\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-30 21:39:10,942\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:12,948\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:14,423\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:15,837\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:15,837\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-30 22:12:52,290\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:53,985\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:55,391\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:56,621\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:56,637\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# for j in np.linspace(0.6, 1, 10):\n",
    "#     config[\"gamma\"] = j\n",
    "#     dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "#     trainer = ppo.PPOTrainer(config = config)\n",
    "#     for i in range(10):\n",
    "#         trainer.train()    \n",
    "#     trainer.save(f\"{dir}/{run}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
