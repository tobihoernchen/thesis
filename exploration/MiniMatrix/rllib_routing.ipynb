{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo, dqn\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import save_hparams\n",
    "from thesis.utils.rllib_utils import rllib_ppo_config, rllib_dqn_config, setup_ray\n",
    "setup_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-03 13:07:44,907\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-09-03 13:07:44,908\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_54704_et4btb11)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-09-03 13:07:51,927\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-03 13:07:59,032\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-03 13:08:00,780\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-03 13:08:02,379\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-09-03 13:08:02,382\tINFO trainable.py:159 -- Trainable.setup took 17.482 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-09-03 13:08:02,386\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "fleetsize = 6\n",
    "max_fleetsize = 10\n",
    "config_args = dict(\n",
    "    withCollisions = True,\n",
    "    reward_target = 1, \n",
    "    reward_distance = -1,\n",
    "    reward_block = -1, \n",
    "    dispatchinginterval=60,\n",
    "    routinginterval = 2,\n",
    ")\n",
    "\n",
    "dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "trainer = ppo.PPOTrainer(rllib_ppo_config(fleetsize, max_fleetsize, config_args, entropy_coeff = 0.0, gamma = 0.95))\n",
    "#trainer = dqn.DQNTrainer(rllib_dqn_config(fleetsize, max_fleetsize, config_args, lin_model=[128,]*8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.restore(\"../../models/Default/6-10-31_08-22_37_34/checkpoint_000120/checkpoint-120\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(500):\n",
    "    for i in range(20):\n",
    "        trainer.train()    \n",
    "    trainer.save(f\"{dir}/{run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = rllib_ppo_config(fleetsize, max_fleetsize, config_args, entropy_coeff = 0.1, gamma = 0.8)\n",
    "# config[\"kl_coeff\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 21:06:39,267\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-08-30 21:06:39,268\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_87962_iq9rk6nm)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-08-30 21:06:46,929\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:52,643\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:54,417\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:56,063\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:56,066\tINFO trainable.py:159 -- Trainable.setup took 16.802 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-30 21:06:56,069\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-30 21:39:10,942\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:12,948\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:14,423\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:15,837\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:15,837\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-30 22:12:52,290\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:53,985\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:55,391\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:56,621\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:56,637\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# for j in np.linspace(0.6, 1, 10):\n",
    "#     config[\"gamma\"] = j\n",
    "#     dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "#     trainer = ppo.PPOTrainer(config = config)\n",
    "#     for i in range(10):\n",
    "#         trainer.train()    \n",
    "#     trainer.save(f\"{dir}/{run}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
