{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo, dqn\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import save_hparams\n",
    "from thesis.utils.rllib_utils import rllib_ppo_config, setup_ray, save, load\n",
    "setup_ray(env = \"Death\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 12:21:33,619\tWARNING ppo.py:386 -- `train_batch_size` (10000) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 2500.\n",
      "2022-10-11 12:21:33,620\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-10-11 12:21:33,621\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_51497_evw37q9d)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-10-11 12:21:40,980\tWARNING env.py:42 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 GPUs on your machine (GPU devices found: [])! If your machine\n    does not have any GPUs, you should set the config keys `num_gpus` and\n    `num_gpus_per_worker` to 0 (they may be set to 1 by default for your\n    particular RL algorithm).\nTo change the config for the `rllib train|rollout` command, use\n  `--config={'[key]': '[value]'}` on the command line.\nTo change the config for `tune.run()` in a script: Modify the python dict\n  passed to `tune.run(config=[...])`.\nTo change the config for an RLlib Trainer instance: Modify the python dict\n  passed to the Trainer's constructor, e.g. `PPOTrainer(config=[...])`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 935\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[0;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[1;34m(self, config, env_creator)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\rllib_dispatching.ipynb Zelle 2\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m config_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     withCollisions \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     reward_target \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdir\u001b[39m, run \u001b[39m=\u001b[39m save_hparams(fleetsize \u001b[39m=\u001b[39m fleetsize, max_fleetsize \u001b[39m=\u001b[39m max_fleetsize, env_args \u001b[39m=\u001b[39m config_args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m trainer \u001b[39m=\u001b[39m ppo\u001b[39m.\u001b[39;49mPPOTrainer(rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, dispatching\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, with_dispatcher\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, lin_model\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[0;32m    863\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m     }\n\u001b[0;32m    868\u001b[0m }\n\u001b[1;32m--> 870\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    871\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[0;32m    872\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\tune\\trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[0;32m    154\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[1;32m--> 156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[0;32m    157\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[0;32m    949\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[1;32m--> 950\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[0;32m    951\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[0;32m    952\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[0;32m    953\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[0;32m    954\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m    955\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    956\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    957\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[0;32m    958\u001b[0m     )\n\u001b[0;32m    959\u001b[0m     \u001b[39m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[0;32m    960\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers_for_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mremote_workers()\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[0;32m    167\u001b[0m     spaces \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[0;32m    171\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mRolloutWorker,\n\u001b[0;32m    172\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[0;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    174\u001b[0m         policy_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[0;32m    175\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m    176\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    177\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[0;32m    178\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[0;32m    179\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:630\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[1;34m(self, cls, env_creator, validate_env, policy_cls, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     extra_python_environs \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mextra_python_environs_for_worker\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 630\u001b[0m worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    631\u001b[0m     env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[0;32m    632\u001b[0m     validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    633\u001b[0m     policy_spec\u001b[39m=\u001b[39;49mpolicies,\n\u001b[0;32m    634\u001b[0m     policy_mapping_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicy_mapping_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    635\u001b[0m     policies_to_train\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicies_to_train\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    636\u001b[0m     tf_session_creator\u001b[39m=\u001b[39;49m(session_creator \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mtf_session_args\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    637\u001b[0m     rollout_fragment_length\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrollout_fragment_length\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    638\u001b[0m     count_steps_by\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcount_steps_by\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    639\u001b[0m     batch_mode\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_mode\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    640\u001b[0m     episode_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mhorizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    641\u001b[0m     preprocessor_pref\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessor_pref\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    642\u001b[0m     sample_async\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msample_async\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    643\u001b[0m     compress_observations\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcompress_observations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    644\u001b[0m     num_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_envs_per_worker\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    645\u001b[0m     observation_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mobservation_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    646\u001b[0m     observation_filter\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mobservation_filter\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    647\u001b[0m     clip_rewards\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_rewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    648\u001b[0m     normalize_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnormalize_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    649\u001b[0m     clip_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    650\u001b[0m     env_config\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    651\u001b[0m     policy_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    652\u001b[0m     worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[0;32m    653\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    654\u001b[0m     recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[0;32m    655\u001b[0m     record_env\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrecord_env\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    656\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[0;32m    657\u001b[0m     log_level\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mlog_level\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    658\u001b[0m     callbacks\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    659\u001b[0m     input_creator\u001b[39m=\u001b[39;49minput_creator,\n\u001b[0;32m    660\u001b[0m     input_evaluation\u001b[39m=\u001b[39;49minput_evaluation,\n\u001b[0;32m    661\u001b[0m     output_creator\u001b[39m=\u001b[39;49moutput_creator,\n\u001b[0;32m    662\u001b[0m     remote_worker_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_worker_envs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    663\u001b[0m     remote_env_batch_wait_ms\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_env_batch_wait_ms\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    664\u001b[0m     soft_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msoft_horizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    665\u001b[0m     no_done_at_end\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mno_done_at_end\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    666\u001b[0m     seed\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39;49m worker_index)\n\u001b[0;32m    667\u001b[0m     \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    668\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    669\u001b[0m     fake_sampler\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mfake_sampler\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    670\u001b[0m     extra_python_environs\u001b[39m=\u001b[39;49mextra_python_environs,\n\u001b[0;32m    671\u001b[0m     spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[0;32m    672\u001b[0m     disable_env_checking\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdisable_env_checking\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    673\u001b[0m )\n\u001b[0;32m    675\u001b[0m \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:612\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[1;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, record_env, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, monitor_path, disable_env_checking)\u001b[0m\n\u001b[0;32m    609\u001b[0m         devices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()))\n\u001b[0;32m    611\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(devices) \u001b[39m<\u001b[39m num_gpus:\n\u001b[1;32m--> 612\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    613\u001b[0m             ERR_MSG_NO_GPUS\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(devices), devices) \u001b[39m+\u001b[39m HOWTO_CHANGE_CONFIG\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    615\u001b[0m \u001b[39m# Warn, if running in local-mode and actual GPUs (not faked) are\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[39m# requested.\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    618\u001b[0m     ray\u001b[39m.\u001b[39mis_initialized()\n\u001b[0;32m    619\u001b[0m     \u001b[39mand\u001b[39;00m ray\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39m_mode() \u001b[39m==\u001b[39m ray\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39mLOCAL_MODE\n\u001b[0;32m    620\u001b[0m     \u001b[39mand\u001b[39;00m num_gpus \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    621\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m policy_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_fake_gpus\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    622\u001b[0m ):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found 0 GPUs on your machine (GPU devices found: [])! If your machine\n    does not have any GPUs, you should set the config keys `num_gpus` and\n    `num_gpus_per_worker` to 0 (they may be set to 1 by default for your\n    particular RL algorithm).\nTo change the config for the `rllib train|rollout` command, use\n  `--config={'[key]': '[value]'}` on the command line.\nTo change the config for `tune.run()` in a script: Modify the python dict\n  passed to `tune.run(config=[...])`.\nTo change the config for an RLlib Trainer instance: Modify the python dict\n  passed to the Trainer's constructor, e.g. `PPOTrainer(config=[...])`.\n"
     ]
    }
   ],
   "source": [
    "fleetsize = 6\n",
    "max_fleetsize = 10\n",
    "config_args = dict(\n",
    "    withCollisions = False,\n",
    "    reward_target = 1, \n",
    "    #reward_distance = -0.1,\n",
    "    reward_block = -1, \n",
    "    reward_invalid=-0.1,\n",
    "    blockTimeout = 120,\n",
    "    reward_acceptance = 5,\n",
    "    reward_declined = -0.2,\n",
    "    dispatchinginterval=240,\n",
    "    ioquote = 0.8,\n",
    "    availability = 0.9,\n",
    "    mttr = 5*60,\n",
    "\n",
    ")\n",
    "\n",
    "dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "trainer = ppo.PPOTrainer(rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations=5, dispatching=True, with_dispatcher=True, lin_model=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 0,\n",
       " 'num_envs_per_worker': 4,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'complete_episodes',\n",
       " 'gamma': 0.98,\n",
       " 'lr': 0.001,\n",
       " 'train_batch_size': 10000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': 'lin_model',\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': 'matrix',\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {'dispatcher': 'stat',\n",
       "  'max_seconds': 3600,\n",
       "  'fleetsize': 6,\n",
       "  'max_fleetsize': 10,\n",
       "  'config_args': {'withCollisions': True,\n",
       "   'reward_target': 1,\n",
       "   'reward_block': -1,\n",
       "   'reward_invalid': -0.1,\n",
       "   'blockTimeout': 120,\n",
       "   'reward_acceptance': 5,\n",
       "   'reward_declined': -0.2,\n",
       "   'dispatchinginterval': 240,\n",
       "   'ioquote': 0.8,\n",
       "   'availability': 0.9,\n",
       "   'mttr': 300}},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': <function thesis.utils.rllib_utils.rllib_ppo_config.<locals>.<lambda>()>,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 1,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {'agv': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={'model': {'custom_model_config': {'fleetsize': 10, 'embed_dim': 32, 'n_stations': 5, 'depth': 6, 'with_action_mask': True, 'with_stations': False}}}),\n",
       "   'dispatcher': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={'model': {'custom_model_config': {'fleetsize': 10, 'embed_dim': 16, 'n_stations': 5, 'depth': 4, 'with_action_mask': True, 'with_agvs': False}}, 'gamma': 0.8, 'lambda': 0.8})},\n",
       "  'policy_mapping_fn': <function thesis.utils.rllib_utils.rllib_ppo_config.<locals>.<lambda>(agent_id, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': ['agv', 'dispatcher'],\n",
       "  'count_steps_by': 'agent_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'disable_env_checking': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 0.98,\n",
       " 'kl_coeff': 0,\n",
       " 'sgd_minibatch_size': 2000,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations=5, dispatching=True, with_dispatcher=True, lin_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.restore(\"../../models/Default/6-10-07_10-10_14_51/checkpoint_000300/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save(trainer, \"agv\", \"../../models/trained_policies/agv_5_9M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load(trainer, \"agv\", \"../../models/trained_policies/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "ModelError[path=, status={'reason': 'MODEL_ERROR', 'next_event_time': 744.9000000000611, 'episode_count': 130, 'step_count': 833, 'model_time': 744.9000000000611}, error=None, msg='']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\rllib_dispatching.ipynb Zelle 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         trainer\u001b[39m.\u001b[39;49mtrain()    \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     trainer\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mdir\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrun\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\tune\\trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warmup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[0;32m    359\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 360\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    361\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1136\u001b[0m, in \u001b[0;36mTrainer.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1133\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1134\u001b[0m             \u001b[39m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m             time\u001b[39m.\u001b[39msleep(\u001b[39m0.5\u001b[39m)\n\u001b[1;32m-> 1136\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m   1138\u001b[0m result \u001b[39m=\u001b[39m step_attempt_results\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mworkers\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers, WorkerSet):\n\u001b[0;32m   1141\u001b[0m     \u001b[39m# Sync filters on workers.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m step_ctx\u001b[39m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[0;32m   1110\u001b[0m     \u001b[39m# Try to train one step.\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m         step_attempt_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_attempt()\n\u001b[0;32m   1113\u001b[0m     \u001b[39m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m     \u001b[39mexcept\u001b[39;00m RayError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1115\u001b[0m         \u001b[39m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[39m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evaluate_this_iter:\n\u001b[1;32m-> 1214\u001b[0m     step_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec_plan_or_training_iteration_fn()\n\u001b[0;32m   1215\u001b[0m \u001b[39m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1217\u001b[0m     \u001b[39m# No parallelism.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mevaluation_parallel_to_training\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:2209\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2207\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2208\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_disable_execution_plan_api\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m-> 2209\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_iteration()\n\u001b[0;32m   2210\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2211\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\ppo\\ppo.py:433\u001b[0m, in \u001b[0;36mPPOTrainer.training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m@ExperimentalAPI\u001b[39m\n\u001b[0;32m    430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_iteration\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResultDict:\n\u001b[0;32m    431\u001b[0m     \u001b[39m# Collect SampleBatches from sample workers until we have a full batch.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_by_agent_steps:\n\u001b[1;32m--> 433\u001b[0m         train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[0;32m    434\u001b[0m             worker_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers, max_agent_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m    435\u001b[0m         )\n\u001b[0;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[0;32m    438\u001b[0m             worker_set\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers, max_env_steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mtrain_batch_size\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    439\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py:96\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[1;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mwhile\u001b[39;00m (max_agent_or_env_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m agent_or_env_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m     90\u001b[0m     max_agent_or_env_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     \u001b[39mand\u001b[39;00m agent_or_env_steps \u001b[39m<\u001b[39m max_agent_or_env_steps\n\u001b[0;32m     92\u001b[0m ):\n\u001b[0;32m     93\u001b[0m     \u001b[39m# No remote workers in the set -> Use local worker for collecting\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[39m# samples.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m worker_set\u001b[39m.\u001b[39mremote_workers():\n\u001b[1;32m---> 96\u001b[0m         sample_batches \u001b[39m=\u001b[39m [worker_set\u001b[39m.\u001b[39;49mlocal_worker()\u001b[39m.\u001b[39;49msample()]\n\u001b[0;32m     97\u001b[0m     \u001b[39m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m         sample_batches \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mget(\n\u001b[0;32m    100\u001b[0m             [worker\u001b[39m.\u001b[39msample\u001b[39m.\u001b[39mremote() \u001b[39mfor\u001b[39;00m worker \u001b[39min\u001b[39;00m worker_set\u001b[39m.\u001b[39mremote_workers()]\n\u001b[0;32m    101\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:825\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39msample_start\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m    820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGenerating sample batch of size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    821\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_fragment_length\n\u001b[0;32m    822\u001b[0m         )\n\u001b[0;32m    823\u001b[0m     )\n\u001b[1;32m--> 825\u001b[0m batches \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_reader\u001b[39m.\u001b[39;49mnext()]\n\u001b[0;32m    826\u001b[0m steps_so_far \u001b[39m=\u001b[39m (\n\u001b[0;32m    827\u001b[0m     batches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcount\n\u001b[0;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_steps_by \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39menv_steps\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    829\u001b[0m     \u001b[39melse\u001b[39;00m batches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39magent_steps()\n\u001b[0;32m    830\u001b[0m )\n\u001b[0;32m    832\u001b[0m \u001b[39m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[39m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py:115\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39m@override\u001b[39m(InputReader)\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SampleBatchType:\n\u001b[1;32m--> 115\u001b[0m     batches \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_data()]\n\u001b[0;32m    116\u001b[0m     batches\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extra_batches())\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batches) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py:288\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m@override\u001b[39m(SamplerInput)\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SampleBatchType:\n\u001b[0;32m    287\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_env_runner)\n\u001b[0;32m    289\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[0;32m    290\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics_queue\u001b[39m.\u001b[39mput(item)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py:721\u001b[0m, in \u001b[0;36m_env_runner\u001b[1;34m(worker, base_env, extra_batch_callback, horizon, normalize_actions, clip_actions, multiple_episodes_in_batch, callbacks, perf_stats, soft_horizon, no_done_at_end, observation_fn, sample_collector, render)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[39m# Return computed actions to ready envs. We also send to envs that have\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[39m# taken off-policy actions; those envs are free to ignore the action.\u001b[39;00m\n\u001b[0;32m    720\u001b[0m t4 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 721\u001b[0m base_env\u001b[39m.\u001b[39;49msend_actions(actions_to_send)\n\u001b[0;32m    722\u001b[0m perf_stats\u001b[39m.\u001b[39menv_wait_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t4\n\u001b[0;32m    724\u001b[0m \u001b[39m# Try to render the env, if required.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py:524\u001b[0m, in \u001b[0;36mMultiAgentEnvWrapper.send_actions\u001b[1;34m(self, action_dict)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEnv \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is already done\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(env_id))\n\u001b[0;32m    523\u001b[0m env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_id]\n\u001b[1;32m--> 524\u001b[0m obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(agent_dict)\n\u001b[0;32m    525\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mNot a multi-agent obs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(rewards, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mNot a multi-agent reward\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py:108\u001b[0m, in \u001b[0;36mPettingZooEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49magent_selection])\n\u001b[0;32m    109\u001b[0m     obs_d \u001b[39m=\u001b[39m {}\n\u001b[0;32m    110\u001b[0m     rew_d \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\envs\\matrix_dispatching_zoo_death.py:168\u001b[0m, in \u001b[0;36mMatrixDispatchingMA_Death.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstepcounter \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffle()\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\envs\\base_alpyne_zoo.py:151\u001b[0m, in \u001b[0;36mBaseAlpyneZoo.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    149\u001b[0m alpyne_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_to_action(action, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection)\n\u001b[0;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msim\u001b[39m.\u001b[39mtake_action(alpyne_action)\n\u001b[1;32m--> 151\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect()\n\u001b[0;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\envs\\base_alpyne_zoo.py:204\u001b[0m, in \u001b[0;36mBaseAlpyneZoo.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mwait_for_completion()\n\u001b[0;32m    205\u001b[0m     alpyne_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_catch_nontraining(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msim\u001b[39m.\u001b[39mget_observation())\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_observation(alpyne_obs)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\model_run.py:118\u001b[0m, in \u001b[0;36mModelRun.wait_for_completion\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mwhile\u001b[39;00m status \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [RunStatus\u001b[39m.\u001b[39mPAUSED, RunStatus\u001b[39m.\u001b[39mCOMPLETED, RunStatus\u001b[39m.\u001b[39mSTOPPED]:\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m==\u001b[39m RunStatus\u001b[39m.\u001b[39mFAILED:\n\u001b[1;32m--> 118\u001b[0m         \u001b[39mraise\u001b[39;00m ModelError(info)\n\u001b[0;32m    119\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolling_period)\n\u001b[0;32m    120\u001b[0m     status, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_state()\n",
      "\u001b[1;31mModelError\u001b[0m: ModelError[path=, status={'reason': 'MODEL_ERROR', 'next_event_time': 744.9000000000611, 'episode_count': 130, 'step_count': 833, 'model_time': 744.9000000000611}, error=None, msg='']"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "for j in range(500):\n",
    "    for i in range(20):\n",
    "        trainer.train()    \n",
    "    trainer.save(f\"{dir}/{run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = trainer.workers.local_worker().get_policy(\"dispatcher\").config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=trainer.workers.local_worker().get_policy(\"agv\").config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = rllib_ppo_config(fleetsize, max_fleetsize, config_args, entropy_coeff = 0.1, gamma = 0.8)\n",
    "# config[\"kl_coeff\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in np.linspace(0.6, 1, 10):\n",
    "#     config[\"gamma\"] = j\n",
    "#     dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "#     trainer = ppo.PPOTrainer(config = config)\n",
    "#     for i in range(10):\n",
    "#         trainer.train()    \n",
    "#     trainer.save(f\"{dir}/{run}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
