{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo, dqn\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import save_hparams\n",
    "from thesis.utils.rllib_utils import rllib_ppo_config, setup_ray\n",
    "setup_ray(env = \"Dispatching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 19:54:58,370\tWARNING ppo.py:386 -- `train_batch_size` (7000) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1750.\n",
      "2022-09-26 19:54:58,372\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-09-26 19:54:58,374\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_82984_n5rllppm)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-09-26 19:55:01,399\tWARNING env.py:42 -- Skipping env checking for this experiment\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[9, 64], expected input with shape [*, 9, 64], but got input of size[32, 14, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 935\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[0;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[1;34m(self, config, env_creator)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\rllib_dispatching.ipynb Zelle 2\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m config_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     withCollisions \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     reward_target \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     routinginterval \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdir\u001b[39m, run \u001b[39m=\u001b[39m save_hparams(fleetsize \u001b[39m=\u001b[39m fleetsize, max_fleetsize \u001b[39m=\u001b[39m max_fleetsize, env_args \u001b[39m=\u001b[39m config_args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/rllib_dispatching.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m trainer \u001b[39m=\u001b[39m ppo\u001b[39m.\u001b[39;49mPPOTrainer(rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, dispatching\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[0;32m    863\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m     }\n\u001b[0;32m    868\u001b[0m }\n\u001b[1;32m--> 870\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    871\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[0;32m    872\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\tune\\trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[0;32m    154\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[1;32m--> 156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[0;32m    157\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[0;32m    949\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[1;32m--> 950\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[0;32m    951\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[0;32m    952\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[0;32m    953\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[0;32m    954\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m    955\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    956\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    957\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[0;32m    958\u001b[0m     )\n\u001b[0;32m    959\u001b[0m     \u001b[39m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[0;32m    960\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers_for_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mremote_workers()\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[0;32m    167\u001b[0m     spaces \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[0;32m    171\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mRolloutWorker,\n\u001b[0;32m    172\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[0;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    174\u001b[0m         policy_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[0;32m    175\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m    176\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    177\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[0;32m    178\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[0;32m    179\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:630\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[1;34m(self, cls, env_creator, validate_env, policy_cls, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     extra_python_environs \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mextra_python_environs_for_worker\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 630\u001b[0m worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    631\u001b[0m     env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[0;32m    632\u001b[0m     validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    633\u001b[0m     policy_spec\u001b[39m=\u001b[39;49mpolicies,\n\u001b[0;32m    634\u001b[0m     policy_mapping_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicy_mapping_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    635\u001b[0m     policies_to_train\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicies_to_train\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    636\u001b[0m     tf_session_creator\u001b[39m=\u001b[39;49m(session_creator \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mtf_session_args\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    637\u001b[0m     rollout_fragment_length\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrollout_fragment_length\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    638\u001b[0m     count_steps_by\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcount_steps_by\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    639\u001b[0m     batch_mode\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_mode\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    640\u001b[0m     episode_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mhorizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    641\u001b[0m     preprocessor_pref\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessor_pref\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    642\u001b[0m     sample_async\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msample_async\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    643\u001b[0m     compress_observations\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcompress_observations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    644\u001b[0m     num_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_envs_per_worker\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    645\u001b[0m     observation_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mobservation_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    646\u001b[0m     observation_filter\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mobservation_filter\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    647\u001b[0m     clip_rewards\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_rewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    648\u001b[0m     normalize_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnormalize_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    649\u001b[0m     clip_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    650\u001b[0m     env_config\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    651\u001b[0m     policy_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    652\u001b[0m     worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[0;32m    653\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    654\u001b[0m     recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[0;32m    655\u001b[0m     record_env\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrecord_env\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    656\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[0;32m    657\u001b[0m     log_level\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mlog_level\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    658\u001b[0m     callbacks\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    659\u001b[0m     input_creator\u001b[39m=\u001b[39;49minput_creator,\n\u001b[0;32m    660\u001b[0m     input_evaluation\u001b[39m=\u001b[39;49minput_evaluation,\n\u001b[0;32m    661\u001b[0m     output_creator\u001b[39m=\u001b[39;49moutput_creator,\n\u001b[0;32m    662\u001b[0m     remote_worker_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_worker_envs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    663\u001b[0m     remote_env_batch_wait_ms\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_env_batch_wait_ms\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    664\u001b[0m     soft_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msoft_horizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    665\u001b[0m     no_done_at_end\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mno_done_at_end\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    666\u001b[0m     seed\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39;49m worker_index)\n\u001b[0;32m    667\u001b[0m     \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    668\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    669\u001b[0m     fake_sampler\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mfake_sampler\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    670\u001b[0m     extra_python_environs\u001b[39m=\u001b[39;49mextra_python_environs,\n\u001b[0;32m    671\u001b[0m     spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[0;32m    672\u001b[0m     disable_env_checking\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdisable_env_checking\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    673\u001b[0m )\n\u001b[0;32m    675\u001b[0m \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:630\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[1;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, record_env, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, monitor_path, disable_env_checking)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    618\u001b[0m     ray\u001b[39m.\u001b[39mis_initialized()\n\u001b[0;32m    619\u001b[0m     \u001b[39mand\u001b[39;00m ray\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39m_mode() \u001b[39m==\u001b[39m ray\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39mLOCAL_MODE\n\u001b[0;32m    620\u001b[0m     \u001b[39mand\u001b[39;00m num_gpus \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    621\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m policy_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_fake_gpus\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    622\u001b[0m ):\n\u001b[0;32m    623\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    624\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are running ray with `local_mode=True`, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigured \u001b[39m\u001b[39m{\u001b[39;00mnum_gpus\u001b[39m}\u001b[39;00m\u001b[39m GPUs to be used! In local mode, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    626\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPolicies are placed on the CPU and the `num_gpus` setting \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    627\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    628\u001b[0m     )\n\u001b[1;32m--> 630\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_policy_map(\n\u001b[0;32m    631\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_dict,\n\u001b[0;32m    632\u001b[0m     policy_config,\n\u001b[0;32m    633\u001b[0m     session_creator\u001b[39m=\u001b[39;49mtf_session_creator,\n\u001b[0;32m    634\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    635\u001b[0m )\n\u001b[0;32m    637\u001b[0m \u001b[39m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[39m# state.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39mfor\u001b[39;00m pol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:1788\u001b[0m, in \u001b[0;36mRolloutWorker._build_policy_map\u001b[1;34m(self, policy_dict, policy_config, session_creator, seed)\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessors[name] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[39m# Create the actual policy object.\u001b[39;00m\n\u001b[1;32m-> 1788\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_map\u001b[39m.\u001b[39;49mcreate_policy(\n\u001b[0;32m   1789\u001b[0m         name, orig_cls, obs_space, act_space, conf, merged_conf\n\u001b[0;32m   1790\u001b[0m     )\n\u001b[0;32m   1792\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_index \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1793\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBuilt policy map: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py:152\u001b[0m, in \u001b[0;36mPolicyMap.create_policy\u001b[1;34m(self, policy_id, policy_cls, observation_space, action_space, config_override, merged_config)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39m# Non-tf: No graph, no session.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     class_ \u001b[39m=\u001b[39m policy_cls\n\u001b[1;32m--> 152\u001b[0m     \u001b[39mself\u001b[39m[policy_id] \u001b[39m=\u001b[39m class_(observation_space, action_space, merged_config)\n\u001b[0;32m    154\u001b[0m \u001b[39m# Store spec (class, obs-space, act-space, and config overrides) such\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m# that the map will be able to reproduce on-the-fly added policies\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39m# from disk.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_specs[policy_id] \u001b[39m=\u001b[39m PolicySpec(\n\u001b[0;32m    158\u001b[0m     policy_class\u001b[39m=\u001b[39mpolicy_cls,\n\u001b[0;32m    159\u001b[0m     observation_space\u001b[39m=\u001b[39mobservation_space,\n\u001b[0;32m    160\u001b[0m     action_space\u001b[39m=\u001b[39maction_space,\n\u001b[0;32m    161\u001b[0m     config\u001b[39m=\u001b[39mconfig_override,\n\u001b[0;32m    162\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\agents\\ppo\\ppo_torch_policy.py:59\u001b[0m, in \u001b[0;36mPPOTorchPolicy.__init__\u001b[1;34m(self, observation_space, action_space, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkl_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mkl_target\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     58\u001b[0m \u001b[39m# TODO: Don't require users to call this manually.\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_loss_from_dummy_batch()\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\policy\\policy.py:904\u001b[0m, in \u001b[0;36mPolicy._initialize_loss_from_dummy_batch\u001b[1;34m(self, auto_remove_unneeded_view_reqs, stats_fn)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dummy_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dummy_batch_from_view_requirements(\n\u001b[0;32m    901\u001b[0m     sample_batch_size\n\u001b[0;32m    902\u001b[0m )\n\u001b[0;32m    903\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy_tensor_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dummy_batch)\n\u001b[1;32m--> 904\u001b[0m actions, state_outs, extra_outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_actions_from_input_dict(\n\u001b[0;32m    905\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dummy_batch, explore\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    906\u001b[0m )\n\u001b[0;32m    907\u001b[0m \u001b[39mfor\u001b[39;00m key, view_req \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mview_requirements\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dummy_batch\u001b[39m.\u001b[39maccessed_keys:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy.py:335\u001b[0m, in \u001b[0;36mTorchPolicy.compute_actions_from_input_dict\u001b[1;34m(self, input_dict, explore, timestep, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m# Calculate RNN sequence lengths.\u001b[39;00m\n\u001b[0;32m    325\u001b[0m seq_lens \u001b[39m=\u001b[39m (\n\u001b[0;32m    326\u001b[0m     torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m    327\u001b[0m         [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(state_batches[\u001b[39m0\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    333\u001b[0m )\n\u001b[1;32m--> 335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_action_helper(\n\u001b[0;32m    336\u001b[0m     input_dict, state_batches, seq_lens, explore, timestep\n\u001b[0;32m    337\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\utils\\threading.py:21\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[1;34m(self, *a, **k)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n\u001b[0;32m     22\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy.py:997\u001b[0m, in \u001b[0;36mTorchPolicy._compute_action_helper\u001b[1;34m(self, input_dict, state_batches, seq_lens, explore, timestep)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m     dist_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_class\n\u001b[1;32m--> 997\u001b[0m     dist_inputs, state_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_dict, state_batches, seq_lens)\n\u001b[0;32m    999\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m   1000\u001b[0m     \u001b[39misinstance\u001b[39m(dist_class, functools\u001b[39m.\u001b[39mpartial)\n\u001b[0;32m   1001\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39missubclass\u001b[39m(dist_class, TorchDistributionWrapper)\n\u001b[0;32m   1002\u001b[0m ):\n\u001b[0;32m   1003\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1004\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`dist_class` (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) not a TorchDistributionWrapper \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1005\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msubclass! Make sure your `action_distribution_fn` or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1006\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`make_model_and_action_dist` return a correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1007\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdistribution class.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(dist_class\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1008\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\ray\\rllib\\models\\modelv2.py:259\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[1;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[0;32m    256\u001b[0m         restored[\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_dict[\u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext():\n\u001b[1;32m--> 259\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(restored, state \u001b[39mor\u001b[39;49;00m [], seq_lens)\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[0;32m    262\u001b[0m     input_dict\u001b[39m.\u001b[39maccessed_keys \u001b[39m=\u001b[39m restored\u001b[39m.\u001b[39maccessed_keys \u001b[39m-\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\policies\\simplified_attention_module.py:127\u001b[0m, in \u001b[0;36mAttentionPolicy.forward\u001b[1;34m(self, obsdict, state, seq_lengths)\u001b[0m\n\u001b[0;32m    125\u001b[0m values \u001b[39m=\u001b[39m main_embedded\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, queries\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_blocks:\n\u001b[1;32m--> 127\u001b[0m     values \u001b[39m=\u001b[39m block(queries, values, values)\n\u001b[0;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    131\u001b[0m actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_net(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\policies\\simplified_attention_module.py:50\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[1;34m(self, q, k, v)\u001b[0m\n\u001b[0;32m     48\u001b[0m attended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(q, k, v, need_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m x \u001b[39m=\u001b[39m v \u001b[39m+\u001b[39m attended\n\u001b[1;32m---> 50\u001b[0m normed1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)\n\u001b[0;32m     51\u001b[0m fedforward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedforward(normed1)\n\u001b[0;32m     52\u001b[0m x \u001b[39m=\u001b[39m fedforward \u001b[39m+\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[0;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   2483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2484\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   2485\u001b[0m     )\n\u001b[1;32m-> 2486\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given normalized_shape=[9, 64], expected input with shape [*, 9, 64], but got input of size[32, 14, 64]"
     ]
    }
   ],
   "source": [
    "fleetsize = 6\n",
    "max_fleetsize = 10\n",
    "config_args = dict(\n",
    "    withCollisions = True,\n",
    "    reward_target = 1, \n",
    "    reward_distance = -0.1,\n",
    "    reward_block = -1, \n",
    "    reward_invalid=-0.3,\n",
    "    reward_acceptance = 5,\n",
    "    reward_declined = -0.5,\n",
    "    dispatchinginterval=120,\n",
    "    routinginterval = 2,\n",
    ")\n",
    "\n",
    "dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "trainer = ppo.PPOTrainer(rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations=5, dispatching=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 09:45:40,507\tINFO trainable.py:588 -- Restored on 127.0.0.1 from checkpoint: ../../models/Default/6-10-25_09-23_26_20/checkpoint_000112/checkpoint-112\n",
      "2022-09-26 09:45:40,509\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 112, '_timesteps_total': None, '_time_total': 80197.72135829926, '_episodes_total': 503}\n"
     ]
    }
   ],
   "source": [
    "trainer.workers.local_worker().get_policy(\"agv\").import_model_from_h5(\"../../models/RLLIB_POLICIES/Routing_V0/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(500):\n",
    "    for i in range(20):\n",
    "        trainer.train()    \n",
    "    trainer.save(f\"{dir}/{run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = rllib_ppo_config(fleetsize, max_fleetsize, config_args, entropy_coeff = 0.1, gamma = 0.8)\n",
    "# config[\"kl_coeff\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 21:06:39,267\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-08-30 21:06:39,268\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_87962_iq9rk6nm)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-08-30 21:06:46,929\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:52,643\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:54,417\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:56,063\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:06:56,066\tINFO trainable.py:159 -- Trainable.setup took 16.802 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-30 21:06:56,069\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-30 21:39:10,942\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:12,948\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:14,423\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:15,837\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 21:39:15,837\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-30 22:12:52,290\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:53,985\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:55,391\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:56,621\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-08-30 22:12:56,637\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# for j in np.linspace(0.6, 1, 10):\n",
    "#     config[\"gamma\"] = j\n",
    "#     dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "#     trainer = ppo.PPOTrainer(config = config)\n",
    "#     for i in range(10):\n",
    "#         trainer.train()    \n",
    "#     trainer.save(f\"{dir}/{run}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
