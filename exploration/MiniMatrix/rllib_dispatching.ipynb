{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo, dqn\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from thesis.utils.utils import save_hparams\n",
    "from thesis.utils.rllib_utils import rllib_ppo_config, setup_ray, save, load\n",
    "setup_ray(env = \"Death\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 00:10:31,429\tWARNING ppo.py:386 -- `train_batch_size` (10000) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 2500.\n",
      "2022-10-19 00:10:31,431\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-10-19 00:10:31,432\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_00730_tc4tb1kb)\n",
      "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n",
      "2022-10-19 00:10:39,725\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-10-19 00:10:49,621\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-10-19 00:10:51,338\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-10-19 00:10:52,989\tWARNING env.py:42 -- Skipping env checking for this experiment\n",
      "2022-10-19 00:10:52,995\tINFO trainable.py:159 -- Trainable.setup took 21.573 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-10-19 00:10:52,999\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "fleetsize = 4\n",
    "max_fleetsize = 10\n",
    "config_args = dict(\n",
    "    withCollisions = True,\n",
    "    reward_target = 1, \n",
    "    #reward_distance = -0.1,\n",
    "    reward_block = -1, \n",
    "    reward_invalid=-0.1,\n",
    "    blockTimeout = 120,\n",
    "    reward_acceptance = 5,\n",
    "    reward_declined = -0.5,\n",
    "    dispatchinginterval=240,\n",
    "    ioquote = 0.8,\n",
    "    availability = 0.9,\n",
    "    mttr = 5*60,\n",
    "\n",
    ")\n",
    "\n",
    "dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "trainer = ppo.PPOTrainer(rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations=5, dispatching=True, with_dispatcher=True, lin_model=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 0,\n",
       " 'num_envs_per_worker': 4,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'complete_episodes',\n",
       " 'gamma': 0.98,\n",
       " 'lr': 3e-05,\n",
       " 'train_batch_size': 10000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': 'lin_model',\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': 'matrix',\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {'dispatcher': None,\n",
       "  'max_seconds': 3600,\n",
       "  'fleetsize': 4,\n",
       "  'max_fleetsize': 10,\n",
       "  'config_args': {'withCollisions': True,\n",
       "   'reward_target': 1,\n",
       "   'reward_block': -1,\n",
       "   'reward_invalid': -0.1,\n",
       "   'blockTimeout': 120,\n",
       "   'reward_acceptance': 5,\n",
       "   'reward_declined': -0.5,\n",
       "   'dispatchinginterval': 240,\n",
       "   'ioquote': 0.8,\n",
       "   'availability': 0.9,\n",
       "   'mttr': 300}},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': <function thesis.utils.rllib_utils.rllib_ppo_config.<locals>.<lambda>()>,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 1,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {'agv': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={'model': {'custom_model_config': {'fleetsize': 10, 'embed_dim': 32, 'n_stations': 5, 'depth': 6, 'with_action_mask': True, 'with_stations': False}}}),\n",
       "   'agv1': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={'model': {'custom_model_config': {'fleetsize': 10, 'embed_dim': 32, 'n_stations': 5, 'depth': 6, 'with_action_mask': True, 'with_stations': False}}}),\n",
       "   'dispatcher': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={'model': {'custom_model_config': {'fleetsize': 10, 'embed_dim': 16, 'n_stations': 5, 'depth': 4, 'with_action_mask': True, 'with_agvs': False}}, 'gamma': 0.8, 'lambda': 0.8}),\n",
       "   'dispatcher1': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={'model': {'custom_model_config': {'fleetsize': 10, 'embed_dim': 16, 'n_stations': 5, 'depth': 4, 'with_action_mask': True, 'with_agvs': False}}, 'gamma': 0.8, 'lambda': 0.8})},\n",
       "  'policy_mapping_fn': <function thesis.utils.rllib_utils.rllib_ppo_config.<locals>.pmfn(agent_id, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': ['agv', 'dispatcher1'],\n",
       "  'count_steps_by': 'agent_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'disable_env_checking': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 0.95,\n",
       " 'kl_coeff': 0,\n",
       " 'sgd_minibatch_size': 2000,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': [[0, 0.0003], [10000000, 3e-08]],\n",
       " 'vf_loss_coeff': 0.5,\n",
       " 'entropy_coeff': 0.01,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.2,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rllib_ppo_config(fleetsize, max_fleetsize, config_args, n_stations=5, dispatching=True,  lin_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 00:10:53,865\tINFO trainable.py:588 -- Restored on 127.0.0.1 from checkpoint: ../../models/Default/4-10-17_10-13_55_03/checkpoint_000060/checkpoint-60\n",
      "2022-10-19 00:10:53,866\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 60, '_timesteps_total': None, '_time_total': 25999.094030857086, '_episodes_total': 288}\n"
     ]
    }
   ],
   "source": [
    "trainer.restore(\"../../models/Default/4-10-17_10-13_55_03/checkpoint_000060/checkpoint-60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save(trainer, \"agv\", \"../../models/trained_policies/agv_wo_coll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load(trainer, \"agv\", \"../../models/trained_policies/agv_wo_coll\")\n",
    "#load(trainer, \"agv1\", \"../../models/trained_policies/agv_wo_coll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "for j in range(500):\n",
    "    for i in range(20):\n",
    "        trainer.train()    \n",
    "    trainer.save(f\"{dir}/{run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = trainer.workers.local_worker().get_policy(\"dispatcher\").config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=trainer.workers.local_worker().get_policy(\"agv\").config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = rllib_ppo_config(fleetsize, max_fleetsize, config_args, entropy_coeff = 0.1, gamma = 0.8)\n",
    "# config[\"kl_coeff\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in np.linspace(0.6, 1, 10):\n",
    "#     config[\"gamma\"] = j\n",
    "#     dir, run = save_hparams(fleetsize = fleetsize, max_fleetsize = max_fleetsize, env_args = config_args)\n",
    "#     trainer = ppo.PPOTrainer(config = config)\n",
    "#     for i in range(10):\n",
    "#         trainer.train()    \n",
    "#     trainer.save(f\"{dir}/{run}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
